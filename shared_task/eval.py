
### a standalone script for evaluation

import argparse

from utils import read_json_line, get_TP_FP_FN, get_label_for_key_from_annotation, get_tagged_label_for_key_from_annotation, get_label_from_tagged_label


parser = argparse.ArgumentParser()
parser.add_argument("-p", "--prediction", help="Path to the predictions generated by the system", type=str, required=True)
parser.add_argument("-g", "--golden", help="Path to the golden labels", type=str, required=True)
args = parser.parse_args()


def main():

    ## read in files
    system_predictions = read_json_line(args.prediction)
    golden_predictions = read_json_line(args.golden)
    golden_predictions_dict = {}
    for each_line in golden_predictions:
        golden_predictions_dict[each_line['id']] = each_line

    # All the questions with interesting annotations start with prefix "part2-" and end with suffix ".Response"
    # Extract all the interesting questions' annotation keys and their corresponding question-tags
    question_keys_and_tags = list()  # list of tuples of the format (<tag>, <dict-key>)
    # Extract the keys and tags from first annotation in the dataset
    dummy_annotation = golden_predictions[0]['annotations']
    for key in dummy_annotation.keys():
        if key.startswith("part2-") and key.endswith(".Response"):
            question_tag = key.replace("part2-", "").replace(".Response", "")
            question_keys_and_tags.append((question_tag, key))
    # Sort the question keys to have a fixed ordering
    question_keys_and_tags.sort(key=lambda tup: tup[0])
    # print(question_keys_and_tags)
    # exit()
    question_tags = [question_tag for question_tag, question_key in question_keys_and_tags]
    question_keys = [question_key for question_tag, question_key in question_keys_and_tags]
    if "gender" in question_tags:
        # Update the question_keys_and_tags
        gender_index = question_tags.index("gender")
        question_tags[gender_index] = "gender_female"
        question_tags.insert(gender_index, "gender_male")
        question_keys.insert(gender_index, question_keys[gender_index])
        question_keys_and_tags = list(zip(question_tags, question_keys))

    ## re-organize by tasks
    task_instances_dict= {question_tag: list() for question_tag, question_key in question_keys_and_tags}
    for each_line in system_predictions:
        annotation = golden_predictions_dict[each_line['id']]['annotations']
        print(annotation)
        for question_tag, question_key in question_keys_and_tags:
            for candidate_chunk in each_line['annotations'][question_key]:
                if question_tag in ["relation", "gender_male", "gender_female", "believe", "binary-relation", "binary-symptoms", "symptoms", "opinion"]:
                    # If the question is a yes/no question. It is for the name candidate chunk
                    special_tagged_chunks = get_tagged_label_for_key_from_annotation(question_key, annotation)
                    assert len(special_tagged_chunks) == 1
                    tagged_label = special_tagged_chunks[0]
                    if tagged_label == "No":
                        tagged_label = "Not Specified"
                    if question_tag in ["gender_male", "gender_female"]:
                        gender = "Male" if question_tag == "gender_male" else "Female"
                        if gender == tagged_label:
                            special_question_label = get_label_from_tagged_label(tagged_label)
                        else:
                            special_question_label = 0
                    else:
                        special_question_label = get_label_from_tagged_label(tagged_label)

                    if question_tag == "opinion":
                        # question_label, tagged_chunks = get_label_for_key_from_annotation("part2-who_cure.Response", annotation, candidate_chunk)
                        tagged_chunks = []
                        if candidate_chunk == "AUTHOR OF THE TWEET":
                            question_label = 1
                            tagged_chunks.append("AUTHOR OF THE TWEET")
                        else:
                            question_label = 0
                    else:
                        question_label, tagged_chunks = get_label_for_key_from_annotation("part2-name.Response", annotation, candidate_chunk)
                    question_label = question_label & special_question_label
                    if question_label == 0:
                        tagged_chunks = []
                else:
                    question_label, tagged_chunks = get_label_for_key_from_annotation(question_key, annotation, candidate_chunk)

                    task_instances_dict[question_tag].append(('', candidate_chunk, '',
                                                             tagged_chunks, question_label))


    ## evaluation
    for each_task in task_instances_dict.keys():
        results = dict()
        F1, P, R, TP, FP, FN = get_TP_FP_FN(task_instances_dict[each_task], [1]*len(task_instances_dict[each_task]), THRESHOLD=0.5)
        # logging.info("New evaluation scores:")
        # logging.info(f"F1: {F1}")
        # logging.info(f"Precision: {P}")
        # logging.info(f"Recall: {R}")
        # logging.info(f"True Positive: {TP}")
        # logging.info(f"False Positive: {FP}")
        # logging.info(f"False Negative: {FN}")
        results["F1"] = F1
        results["P"] = P
        results["R"] = R
        results["TP"] = TP
        results["FP"] = FP
        results["FN"] = FN
        N = TP + FN
        results["N"] = N
        print(each_task)
        print(results)


if __name__ == '__main__':
    main()